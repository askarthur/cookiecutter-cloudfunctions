{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cookiecutter Google Function Cookiecutter template for a Google Function python project. Powered by Poetry, GitHub actions, and Google Cloud Platform. Introduction Welcome to cookiecutter-cloudfunctions! This project was created because we wanted an easy-to-configure template for using Google Functions. In the current state, there aren't any cookiecutters focusing on deploying a Google Function using GitHub Actions and Cloud Build for continuous integration and continuous deployment (CI/CD). We hope this will allow developers to create, test, and deploy their pipeline(s) in an easy-to-use and easy-to-maintain way. File structure All code should go in the directory with your given project-name. The packaging works with a single file of code or multiple modules nested within sub-directories. Currently, we don't have a public example of multi-file setup but we're working on it. Features This template has the following features: pytest : Unit and coverage testing flake8 and pylint : Python style checks black : Auto-formatted code mypy : Type checking Poetry : Depedency management GitHub Actions : Automated CI checks This is a simple list, for a deep-dive into why and how each feature is used visit feature explanation . If already familiar or not interested, continue to Getting started .","title":"Introduction"},{"location":"#cookiecutter-google-function","text":"Cookiecutter template for a Google Function python project. Powered by Poetry, GitHub actions, and Google Cloud Platform.","title":"Cookiecutter Google Function"},{"location":"#introduction","text":"Welcome to cookiecutter-cloudfunctions! This project was created because we wanted an easy-to-configure template for using Google Functions. In the current state, there aren't any cookiecutters focusing on deploying a Google Function using GitHub Actions and Cloud Build for continuous integration and continuous deployment (CI/CD). We hope this will allow developers to create, test, and deploy their pipeline(s) in an easy-to-use and easy-to-maintain way.","title":"Introduction"},{"location":"#file-structure","text":"All code should go in the directory with your given project-name. The packaging works with a single file of code or multiple modules nested within sub-directories. Currently, we don't have a public example of multi-file setup but we're working on it.","title":"File structure"},{"location":"#features","text":"This template has the following features: pytest : Unit and coverage testing flake8 and pylint : Python style checks black : Auto-formatted code mypy : Type checking Poetry : Depedency management GitHub Actions : Automated CI checks This is a simple list, for a deep-dive into why and how each feature is used visit feature explanation . If already familiar or not interested, continue to Getting started .","title":"Features"},{"location":"advanced/feature_explanation/","text":"Feature Explanation Pytest Pytest is the package used for unit and coverage testing within the cookiecutter. We've chosen it over unittest as we find it easier to use and understand the results of. In addition, we like the native fixture functionality of pytest and the pytest-cov extension (used for coverage testing). Flake8 and Pylint Flake8 and Pylint are both linters developed by the Python Code Quality Authority ( PyCQA ). As pylint is typically more exhaustive we typically only use it for less fault-tolerant code. If you want a more relaxed linter (as we typically use with our repos), use flake8 to get good linting with slightly faster development speed. Black Black is my go-to code formatter. Although annoying in the beginning, we've grown to love black as it makes git diffs 100x easier. With strict and opionated formatting, it does all the tedious formatting you're too lazy to do and leaves your pull requests with only the important changes being highlighted. Mypy Mypy is the official checker for type-hinting which was added in Python 3.6. we think type-hinting allows for way better readability of Python code. Additionally, it makes sure that variables being passed throughout your program are being correctly accounted for in terms of their type (passing a string of \"1\" when it should be 1 can cause issues down the line). Poetry Poetry is the dependency manager and packager for the cookiecutter. As we've grown frusturated with older tools for Python packaging (setuptools, tox, etc.) we decided to give Poetry a try and it was 100% worth it. It comprises all the necessary settings into a single pyproject.toml file (instead of setup.py/cfg , requirements.txt , MANIFEST.in , etc.) which has growing use by the above packages (with pylint, pytest, and black already supporting). Poetry is one of the main reasons the CI files are short and sweet and the repository is not overrun with packaging files. Github Actions GitHub Actions is still pretty new to the CI space and is probably the biggest split (with Poetry) we use from most Python repositories. The main reason for using GitHub Actions is it's free (up to 2,000/minutes a month) and it's where my code exists. A lot of open-source projects use TravisCI or other services which we think adds complexity to developing and maintaining a project. Development process Although this isn't a feature per-se, we think it's important to address as it's how we built the cookiecutter structure. The main drive behind having the CI checks on the main and development branches is so these are typically \"protected\" from having bad code.","title":"Feature explanation"},{"location":"advanced/feature_explanation/#feature-explanation","text":"","title":"Feature Explanation"},{"location":"advanced/feature_explanation/#pytest","text":"Pytest is the package used for unit and coverage testing within the cookiecutter. We've chosen it over unittest as we find it easier to use and understand the results of. In addition, we like the native fixture functionality of pytest and the pytest-cov extension (used for coverage testing).","title":"Pytest"},{"location":"advanced/feature_explanation/#flake8-and-pylint","text":"Flake8 and Pylint are both linters developed by the Python Code Quality Authority ( PyCQA ). As pylint is typically more exhaustive we typically only use it for less fault-tolerant code. If you want a more relaxed linter (as we typically use with our repos), use flake8 to get good linting with slightly faster development speed.","title":"Flake8 and Pylint"},{"location":"advanced/feature_explanation/#black","text":"Black is my go-to code formatter. Although annoying in the beginning, we've grown to love black as it makes git diffs 100x easier. With strict and opionated formatting, it does all the tedious formatting you're too lazy to do and leaves your pull requests with only the important changes being highlighted.","title":"Black"},{"location":"advanced/feature_explanation/#mypy","text":"Mypy is the official checker for type-hinting which was added in Python 3.6. we think type-hinting allows for way better readability of Python code. Additionally, it makes sure that variables being passed throughout your program are being correctly accounted for in terms of their type (passing a string of \"1\" when it should be 1 can cause issues down the line).","title":"Mypy"},{"location":"advanced/feature_explanation/#poetry","text":"Poetry is the dependency manager and packager for the cookiecutter. As we've grown frusturated with older tools for Python packaging (setuptools, tox, etc.) we decided to give Poetry a try and it was 100% worth it. It comprises all the necessary settings into a single pyproject.toml file (instead of setup.py/cfg , requirements.txt , MANIFEST.in , etc.) which has growing use by the above packages (with pylint, pytest, and black already supporting). Poetry is one of the main reasons the CI files are short and sweet and the repository is not overrun with packaging files.","title":"Poetry"},{"location":"advanced/feature_explanation/#github-actions","text":"GitHub Actions is still pretty new to the CI space and is probably the biggest split (with Poetry) we use from most Python repositories. The main reason for using GitHub Actions is it's free (up to 2,000/minutes a month) and it's where my code exists. A lot of open-source projects use TravisCI or other services which we think adds complexity to developing and maintaining a project.","title":"Github Actions"},{"location":"advanced/feature_explanation/#development-process","text":"Although this isn't a feature per-se, we think it's important to address as it's how we built the cookiecutter structure. The main drive behind having the CI checks on the main and development branches is so these are typically \"protected\" from having bad code.","title":"Development process"},{"location":"getting-started/configure_github_repo/","text":"Configure GitHub repository Setup branches Now that we've pushed our repository to GitHub we'll see a main branch with all the code but you rarely want to push code directly to main. To mitigate this, lets create a development branch for which to actively develop code and then move it to main once we think it's safe for production. To do so, simply: Click on the \"main\" dropdown button your repo's homepage Type in \"development\" in the search bar Select \"Create branch: development from main\". Note if 'development' is not used. You must change the branch reference in certain files within the '.github/workflows' directory for CI checks to work. As seen with main, you'll see Actions being run for development. This is because main and development are the two branches enabled for actions. To alter this, checkout out Line 6 in your .github/workflows/code_quality_checks.yml file. That is the base needed for the CI system to work. If you wish to add further protection to your repository visit the Settings -> Branches section of your repo. This will allow you to add things like protected branches and/or required reviews.","title":"Configure GitHub repository"},{"location":"getting-started/configure_github_repo/#configure-github-repository","text":"","title":"Configure GitHub repository"},{"location":"getting-started/configure_github_repo/#setup-branches","text":"Now that we've pushed our repository to GitHub we'll see a main branch with all the code but you rarely want to push code directly to main. To mitigate this, lets create a development branch for which to actively develop code and then move it to main once we think it's safe for production. To do so, simply: Click on the \"main\" dropdown button your repo's homepage Type in \"development\" in the search bar Select \"Create branch: development from main\". Note if 'development' is not used. You must change the branch reference in certain files within the '.github/workflows' directory for CI checks to work. As seen with main, you'll see Actions being run for development. This is because main and development are the two branches enabled for actions. To alter this, checkout out Line 6 in your .github/workflows/code_quality_checks.yml file. That is the base needed for the CI system to work. If you wish to add further protection to your repository visit the Settings -> Branches section of your repo. This will allow you to add things like protected branches and/or required reviews.","title":"Setup branches"},{"location":"getting-started/connect_to_remote_repo/","text":"Connect to remote repository on GitHub Initialize local repository Now lets initialize a git repository within your local project. First, move into the directory of the project. $ cd project-name/ Next, follow poetry installation instructions. Note that the terminal will have to be restarted for the installation to be noticed. Then, install base dependencies: $ pip install poetry && poetry install Lastly, initialize the repo. $ git init This will create a git repository within the project. Don't do anything just yet with it. Create remote repository on GitHub If it hasn't already been created, we'll have to create a remote repository on GitHub. To do so, follow the steps: Go to GitHub's create new repository page . Fill in the name and description with the values given at project creation. Make your choice of Public or Private. Leave all other boxes unchecked (they're all populated by the cookiecutter). Sync local repository with GitHub Add remote link Let's start with syncing the newly created remote repository to the local repostiory so it knows where to push changes to. To do so run: $ git remote add origin link_to_repo Make sure to replace link_to_repo with your repo's url. It can be found on your repository's home page under the green \"Code\" dropdown menu button. Push local files to remote Now that the remote repository is setup let's get all the code generated by the cookiecutter there. First let's add all the project files to the local repository (remember we only initalized an empty repository): $ git add . Next, commit those changes: $ git commit -m \"Initialize repo\" Then, we name the current branch (with the files) to main : $ git branch -M main Note if main is not used. You must change the branch reference in certain files within the '.github/workflows' directory for CI checks to work. Finally, lets push it to GitHub: $ git push -u origin main That's it! Now you should see your GitHub repository with all the starter files. In addition, check out the \"Actions\" section of the repository to see the CI checks (docs deployment and code quality) being run. Once these are done, you'll see a gh-pages branch was created with odd files. Don't change them as it's the automatically generated documentation files. Continue on to see how to configure your GitHub repository to work optimally with the cookiecutter setup!","title":"Connect to remote repository"},{"location":"getting-started/connect_to_remote_repo/#connect-to-remote-repository-on-github","text":"","title":"Connect to remote repository on GitHub"},{"location":"getting-started/connect_to_remote_repo/#initialize-local-repository","text":"Now lets initialize a git repository within your local project. First, move into the directory of the project. $ cd project-name/ Next, follow poetry installation instructions. Note that the terminal will have to be restarted for the installation to be noticed. Then, install base dependencies: $ pip install poetry && poetry install Lastly, initialize the repo. $ git init This will create a git repository within the project. Don't do anything just yet with it.","title":"Initialize local repository"},{"location":"getting-started/connect_to_remote_repo/#create-remote-repository-on-github","text":"If it hasn't already been created, we'll have to create a remote repository on GitHub. To do so, follow the steps: Go to GitHub's create new repository page . Fill in the name and description with the values given at project creation. Make your choice of Public or Private. Leave all other boxes unchecked (they're all populated by the cookiecutter).","title":"Create remote repository on GitHub"},{"location":"getting-started/connect_to_remote_repo/#sync-local-repository-with-github","text":"","title":"Sync local repository with GitHub"},{"location":"getting-started/connect_to_remote_repo/#add-remote-link","text":"Let's start with syncing the newly created remote repository to the local repostiory so it knows where to push changes to. To do so run: $ git remote add origin link_to_repo Make sure to replace link_to_repo with your repo's url. It can be found on your repository's home page under the green \"Code\" dropdown menu button.","title":"Add remote link"},{"location":"getting-started/connect_to_remote_repo/#push-local-files-to-remote","text":"Now that the remote repository is setup let's get all the code generated by the cookiecutter there. First let's add all the project files to the local repository (remember we only initalized an empty repository): $ git add . Next, commit those changes: $ git commit -m \"Initialize repo\" Then, we name the current branch (with the files) to main : $ git branch -M main Note if main is not used. You must change the branch reference in certain files within the '.github/workflows' directory for CI checks to work. Finally, lets push it to GitHub: $ git push -u origin main That's it! Now you should see your GitHub repository with all the starter files. In addition, check out the \"Actions\" section of the repository to see the CI checks (docs deployment and code quality) being run. Once these are done, you'll see a gh-pages branch was created with odd files. Don't change them as it's the automatically generated documentation files. Continue on to see how to configure your GitHub repository to work optimally with the cookiecutter setup!","title":"Push local files to remote"},{"location":"getting-started/create_local_project/","text":"Getting started Create local project Setup cookiecutter To start, install the latest Cookiecutter if you haven't installed it yet (this requires Cookiecutter 1.4.0 or higher): $ pip install -U cookiecutter>=1.4.0 Create local project via cookiecutter Now you can use cookiecutter to generate your Python project: $ cookiecutter https://github.com/askarthur/cookiecutter-cloudfunctions.git Now you should have a project directory for your code! Continue on to see how to get it onto GitHub.","title":"Create local project"},{"location":"getting-started/create_local_project/#getting-started","text":"","title":"Getting started"},{"location":"getting-started/create_local_project/#create-local-project","text":"","title":"Create local project"},{"location":"getting-started/create_local_project/#setup-cookiecutter","text":"To start, install the latest Cookiecutter if you haven't installed it yet (this requires Cookiecutter 1.4.0 or higher): $ pip install -U cookiecutter>=1.4.0","title":"Setup cookiecutter"},{"location":"getting-started/create_local_project/#create-local-project-via-cookiecutter","text":"Now you can use cookiecutter to generate your Python project: $ cookiecutter https://github.com/askarthur/cookiecutter-cloudfunctions.git Now you should have a project directory for your code! Continue on to see how to get it onto GitHub.","title":"Create local project via cookiecutter"},{"location":"getting-started/deploy_to_gcp/","text":"Deploy to GCP Check Trigger requirements Because some triggers are too complex to fully configure from the cookiecutter template, first check the Triggers section for your trigger type and read over any additional steps needed. Google Build First, go to Google Build and select the project to use. Go to the \"Trigger\" page, select \"Create Trigger\" and follow the process with these instructions: Use \"Push to a branch\" and it should properly autoselect main once the repository is connected Connect the necessary repository Leave the autoselected build configuration as cloudbuild.yaml Select \"Create\" Now you have a build trigger, before you can run anything you'll have to update the settings. Click the \"Settings\" tab and enable the role of \"Cloud Functions Developer\". If your function needs other permissions like Storage, Firestore, etc. you'll have to set that up in IAM . Note the service account on the \"Settings\" tab is what you want to permission. If you would like to run the trigger without having to push to main, go to the \"Trigger\" tab and click \"Run\" on the newly created trigger.","title":"Deploy to GCP"},{"location":"getting-started/deploy_to_gcp/#deploy-to-gcp","text":"","title":"Deploy to GCP"},{"location":"getting-started/deploy_to_gcp/#check-trigger-requirements","text":"Because some triggers are too complex to fully configure from the cookiecutter template, first check the Triggers section for your trigger type and read over any additional steps needed.","title":"Check Trigger requirements"},{"location":"getting-started/deploy_to_gcp/#google-build","text":"First, go to Google Build and select the project to use. Go to the \"Trigger\" page, select \"Create Trigger\" and follow the process with these instructions: Use \"Push to a branch\" and it should properly autoselect main once the repository is connected Connect the necessary repository Leave the autoselected build configuration as cloudbuild.yaml Select \"Create\" Now you have a build trigger, before you can run anything you'll have to update the settings. Click the \"Settings\" tab and enable the role of \"Cloud Functions Developer\". If your function needs other permissions like Storage, Firestore, etc. you'll have to set that up in IAM . Note the service account on the \"Settings\" tab is what you want to permission. If you would like to run the trigger without having to push to main, go to the \"Trigger\" tab and click \"Run\" on the newly created trigger.","title":"Google Build"},{"location":"triggers/cloud_firestore/","text":"Cloud Firestore As a Cloud Firestore trigger is a little more complex, there are some variables that are left empty by default and MUST be filled in manually. Official Docs For more information about Cloud Firestore triggers, check out the official docs . Cloudbuild.yaml Because you're using a Cloud Storage trigger, the --trigger-resource and --trigger-event are added WITHOUT the value being set. These are required to be set manually. Check the above docs for examples of what is needed. main.py The run_pipeline is a simple function created to simply read in the request, unpackage it, and call your pipeline with the arguments. This is the function that will be called when the trigger is run. Note we only use the event variable by default and don't interact with context . While it can be useful, it didn't seem necessary to be used by default.","title":"Cloud Firestore"},{"location":"triggers/cloud_firestore/#cloud-firestore","text":"As a Cloud Firestore trigger is a little more complex, there are some variables that are left empty by default and MUST be filled in manually.","title":"Cloud Firestore"},{"location":"triggers/cloud_firestore/#official-docs","text":"For more information about Cloud Firestore triggers, check out the official docs .","title":"Official Docs"},{"location":"triggers/cloud_firestore/#cloudbuildyaml","text":"Because you're using a Cloud Storage trigger, the --trigger-resource and --trigger-event are added WITHOUT the value being set. These are required to be set manually. Check the above docs for examples of what is needed.","title":"Cloudbuild.yaml"},{"location":"triggers/cloud_firestore/#mainpy","text":"The run_pipeline is a simple function created to simply read in the request, unpackage it, and call your pipeline with the arguments. This is the function that will be called when the trigger is run. Note we only use the event variable by default and don't interact with context . While it can be useful, it didn't seem necessary to be used by default.","title":"main.py"},{"location":"triggers/cloud_storage/","text":"Cloud Storage As a Cloud Storage trigger is a little more complex, there are some variables that are left empty by default and MUST be filled in manually. Official Docs For more information about Cloud Storage triggers, check out the official docs . Cloudbuild.yaml Because you're using a Cloud Storage trigger, the --trigger-resource and --trigger-event are added WITHOUT the value being set. These are required to be set manually. Check the above docs for examples of what is needed. main.py The run_pipeline is a simple function created to simply read in the request, unpackage it, and call your pipeline with the arguments. This is the function that will be called when the trigger is run. Note we only use the event variable by default and don't interact with context . While it can be useful, it didn't seem necessary to be used by default. Note that the Cloud Storage used bucket change as the lowest granularity. This means that you can't focus on just a certain file or file type. This can be done in the run_pipeline function by accessing the event[\"name\"] attribute and checking if it matches the file you want to trigger on.","title":"Cloud Storage"},{"location":"triggers/cloud_storage/#cloud-storage","text":"As a Cloud Storage trigger is a little more complex, there are some variables that are left empty by default and MUST be filled in manually.","title":"Cloud Storage"},{"location":"triggers/cloud_storage/#official-docs","text":"For more information about Cloud Storage triggers, check out the official docs .","title":"Official Docs"},{"location":"triggers/cloud_storage/#cloudbuildyaml","text":"Because you're using a Cloud Storage trigger, the --trigger-resource and --trigger-event are added WITHOUT the value being set. These are required to be set manually. Check the above docs for examples of what is needed.","title":"Cloudbuild.yaml"},{"location":"triggers/cloud_storage/#mainpy","text":"The run_pipeline is a simple function created to simply read in the request, unpackage it, and call your pipeline with the arguments. This is the function that will be called when the trigger is run. Note we only use the event variable by default and don't interact with context . While it can be useful, it didn't seem necessary to be used by default. Note that the Cloud Storage used bucket change as the lowest granularity. This means that you can't focus on just a certain file or file type. This can be done in the run_pipeline function by accessing the event[\"name\"] attribute and checking if it matches the file you want to trigger on.","title":"main.py"},{"location":"triggers/http/","text":"HTTP As HTTP is the easiest way to trigger a function, there isn't much additional setup required. All the configurations can be run as is but we'll point to a few things to keep in mind. Official Docs For more information about HTTP triggers, check out the official docs . Cloudbuild.yaml Because you're using an HTTP trigger, the --trigger-http flag is added. If this is removed, Cloud Build won't be able to properly create the function for the initial build. main.py The run_pipeline is a simple function created to simply read in the request, unpackage it, and call your pipeline with the arguments. This is the function that will be called when the trigger is run. For more information about the request check the flask request object docs .","title":"HTTP"},{"location":"triggers/http/#http","text":"As HTTP is the easiest way to trigger a function, there isn't much additional setup required. All the configurations can be run as is but we'll point to a few things to keep in mind.","title":"HTTP"},{"location":"triggers/http/#official-docs","text":"For more information about HTTP triggers, check out the official docs .","title":"Official Docs"},{"location":"triggers/http/#cloudbuildyaml","text":"Because you're using an HTTP trigger, the --trigger-http flag is added. If this is removed, Cloud Build won't be able to properly create the function for the initial build.","title":"Cloudbuild.yaml"},{"location":"triggers/http/#mainpy","text":"The run_pipeline is a simple function created to simply read in the request, unpackage it, and call your pipeline with the arguments. This is the function that will be called when the trigger is run. For more information about the request check the flask request object docs .","title":"main.py"},{"location":"triggers/pub_sub/","text":"Pub/Sub As Pub/Sub is can be used for many reasons, there isn't much configured by default. All the configurations can be run as is but we'll point to a few things to keep in mind. Official Docs For more information about Pub/Sub triggers, check out the official docs . For more information about using it with Cloud Scheduler, check out the official docs Cloudbuild.yaml Because you're using a Pub/Sub trigger, the --trigger-topic flag is added with the value defaulting to your project_slug . If this is removed, Cloud Build won't be able to properly create the function for the initial build. To update this, simply change the value and redeploy. main.py The run_pipeline is a simple function created to simply read in the request, unpackage it, and call your pipeline with the arguments. This is the function that will be called when the trigger is run. Note we only use the event variable by default and don't interact with context . While it can be useful, it didn't seem necessary to be used by default.","title":"Pub/Sub"},{"location":"triggers/pub_sub/#pubsub","text":"As Pub/Sub is can be used for many reasons, there isn't much configured by default. All the configurations can be run as is but we'll point to a few things to keep in mind.","title":"Pub/Sub"},{"location":"triggers/pub_sub/#official-docs","text":"For more information about Pub/Sub triggers, check out the official docs . For more information about using it with Cloud Scheduler, check out the official docs","title":"Official Docs"},{"location":"triggers/pub_sub/#cloudbuildyaml","text":"Because you're using a Pub/Sub trigger, the --trigger-topic flag is added with the value defaulting to your project_slug . If this is removed, Cloud Build won't be able to properly create the function for the initial build. To update this, simply change the value and redeploy.","title":"Cloudbuild.yaml"},{"location":"triggers/pub_sub/#mainpy","text":"The run_pipeline is a simple function created to simply read in the request, unpackage it, and call your pipeline with the arguments. This is the function that will be called when the trigger is run. Note we only use the event variable by default and don't interact with context . While it can be useful, it didn't seem necessary to be used by default.","title":"main.py"}]}